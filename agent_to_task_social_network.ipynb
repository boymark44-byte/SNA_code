{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from networkx_viewer import Viewer\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict\n",
    "\n",
    "#from model import spcall\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(G, measures, measure_name):\n",
    "  #* https://stackoverflow.com/a/52013202\n",
    "  #* https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html\n",
    "  #* https://www.datacamp.com/community/tutorials/social-network-analysis-python\n",
    "\n",
    "  #* Create two lists of edges based on their weight.\n",
    "  #* 'elarge' contains edges with weight greater than 5.\n",
    "  #* 'esmall' contains edges with weight less than or equal to 5.\n",
    "  elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 5]\n",
    "  esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 5]\n",
    "\n",
    "  #* Generate a spring layout for the graph.\n",
    "  pos = nx.spring_layout(G)\n",
    "\n",
    "  #* Set the size of each node based on its corresponding measure value.\n",
    "  node_size = [v * 1000 for v in measures.values()]\n",
    "\n",
    "  #* Draw the nodes of the graph with their size and color determined by the measure values.\n",
    "  #* The color map 'plt.cm.plasma' is used for coloring the nodes.\n",
    "  nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size, \n",
    "                                  cmap=plt.cm.plasma,\n",
    "                                  node_color=list(measures.values()),\n",
    "                                  nodelist=measures.keys())\n",
    "\n",
    "  #* Set the color normalization of the nodes to be logarithmic.\n",
    "  nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n",
    "  \n",
    "  #* Draw the edges of the graph.\n",
    "  edges = nx.draw_networkx_edges(G, pos)\n",
    "  \n",
    "  #* Draw the 'elarge' and 'esmall' edges with different styles.\n",
    "  #* The 'elarge' edges are drawn with a width of 2.\n",
    "  #* The 'esmall' edges are drawn with a width of 2, transparency of 0.5, blue color, and dashed style.\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=elarge, width=2)\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=esmall, width=2, alpha=0.5, edge_color='blue', style='dashed')\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, blue color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G, pos, font_size=10, font_color='blue', font_family='sans-serif')\n",
    "  \n",
    "  #* Set the title of the plot, add a color bar, turn off the axis, and display the plot.\n",
    "  plt.title(measure_name)\n",
    "  plt.colorbar(nodes)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_G(G, measures):\n",
    "  #* Define two lists of edges based on their weight.\n",
    "  #* 'elarge' contains edges with weight greater than 5.\n",
    "  #* 'esmall' contains edges with weight less than or equal to 5.\n",
    "  elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 5]\n",
    "  esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 5]\n",
    "\n",
    "  #* Generate a spring layout for the graph.\n",
    "  #* This layout treats edges as springs holding nodes close, while treating nodes as repelling objects.\n",
    "  pos = nx.spring_layout(G)\n",
    "  \n",
    "  #* Set the size of each node based on its corresponding measure value.\n",
    "  #* The size is multiplied by 1000 for better visibility.\n",
    "  node_size = [v * 1000 for v in measures.values()]\n",
    "\n",
    "  #* Draw the nodes of the graph with their size and color determined by the measure values.\n",
    "  #* The color map 'plt.cm.plasma' is used for coloring the nodes.\n",
    "  nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,\n",
    "                                cmap=plt.cm.plasma,\n",
    "                                node_color=list(measures.values()),\n",
    "                                nodelist=measures.keys())\n",
    "\n",
    "  #* Set the color normalization of the nodes to be logarithmic.\n",
    "  #* This can be useful if the measure values vary widely.\n",
    "  nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n",
    "\n",
    "  #* Draw the nodes of the graph again with a fixed size of 50 and a color map of 'plt.cm.plasma'.\n",
    "  nx.draw_networkx_nodes(G, pos, node_size=50, cmap=plt.cm.plasma)\n",
    "\n",
    "  #* Draw the 'elarge' and 'esmall' edges with different styles.\n",
    "  #* The 'elarge' edges are drawn with a width of 2.\n",
    "  #* The 'esmall' edges are drawn with a width of 2, transparency of 0.5, blue color, and dashed style.\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=elarge, width=2)\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=esmall, width=2, alpha=0.5, edge_color='blue', style='dashed')\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, black color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G ,pos, font_size=10, font_color='black', font_family='sans-serif')\n",
    "\n",
    "  #* Turn off the axis and display the plot.\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Gp(G, measures):\n",
    "  # Set the figure size to make the plot high-definition.\n",
    "  plt.figure(figsize=(50, 50), dpi=300)\n",
    "\n",
    "  #* Define two lists of edges based on their weight.\n",
    "  #* 'elarge' contains edges with weight greater than 5.\n",
    "  #* 'esmall' contains edges with weight less than or equal to 5.\n",
    "  elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 5]\n",
    "  esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 5]\n",
    "\n",
    "  #* Generate a spring layout for the graph.\n",
    "  #* This layout treats edges as springs holding nodes close, while treating nodes as repelling objects.\n",
    "  pos = nx.spring_layout(G, iterations=13, scale=300, seed=1234)\n",
    "\n",
    "  #* Set the size of each node based on its corresponding measure value.\n",
    "  #* The size is multiplied by 1000 for better visibility.\n",
    "  node_size = [v * 1000 for v in measures.values()]\n",
    "\n",
    "  #* Draw the nodes of the graph with their size and color determined by the measure values.\n",
    "  #* The color map 'plt.cm.plasma' is used for coloring the nodes.\n",
    "  nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size, cmap=plt.cm.plasma, \n",
    "                                  node_color=list(measures.values()),\n",
    "                                  nodelist=measures.keys())\n",
    "\n",
    "  #* Set the color normalization of the nodes to be logarithmic.\n",
    "  #* This can be useful if the measure values vary widely.\n",
    "  nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n",
    "\n",
    "  #* Create a color map based on the node labels.\n",
    "  #* Different labels are mapped to different colors.\n",
    "  color_map = []\n",
    "  for node in G:\n",
    "    if 'fac' in node:\n",
    "        color_map.append('blue')\n",
    "    elif 'stu' in node:\n",
    "        color_map.append('green')\n",
    "    elif 'adm' in node:\n",
    "        color_map.append('yellow')\n",
    "    elif 'sub' in node:\n",
    "        color_map.append('orange')\n",
    "    elif 'isp' in node:\n",
    "        color_map.append('red')\n",
    "    elif 'bis' in node:\n",
    "        color_map.append('purple')\n",
    "    elif 'par' in node:\n",
    "        color_map.append('black')\n",
    "    elif 'sup' in node:\n",
    "        color_map.append('aqua')\n",
    "    elif node in ['grade', 'assignment', 'Bulletin Board', 're-assign', 'enroll', 'register', 'transfer', 'drop']:\n",
    "        color_map.append('gray')  # white for the specific nodes\n",
    "    else:\n",
    "        print (node)\n",
    "\n",
    "  #* Draw the nodes of the graph again with a fixed size of 10 and a color map based on the node labels.\n",
    "  nx.draw_networkx_nodes(G, pos, node_size=10, node_color=color_map, cmap=plt.cm.plasma)\n",
    "\n",
    "  #* Draw the 'elarge' and 'esmall' edges with different styles.\n",
    "  #* The 'elarge' edges are drawn with a width of 2 and gray color.\n",
    "  #* The 'esmall' edges are drawn with a width of 2, transparency of 0.5, gray color, and dashed style.\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=elarge, edge_color='gray', width=1)\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=esmall, width=1, alpha=0.5, edge_color='gray', style='dashed')\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, gray color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G, pos, font_size=3, font_color='black', font_family='sans-serif')\n",
    "\n",
    "  #* Turn off the axis and display the plot.\n",
    "  plt.axis('off')\n",
    "\n",
    "  #* Increase the DPI to 300 for a high-quality plot.\n",
    "  plt.savefig(\"network.png\", dpi=300)\n",
    "  \n",
    "  #* Display the plot \n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_G2(G):\n",
    "  #* Generate a spring layout for the graph.\n",
    "  pos = nx.spring_layout(G)\n",
    "\n",
    "  #* Draw the graph using NetworkX's built-in draw function.\n",
    "  nx.draw_networkx(G)\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, gray color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G, pos, font_size=10, font_color='gray', font_family='sans-serif')\n",
    "\n",
    "  #* Turn off the axis and display the plot.\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeandrender(filename, centrality):\n",
    "  #* Extract the measure from the centrality dictionary.\n",
    "  cmeasure = centrality[\"measure\"]\n",
    "  \n",
    "  #* Sort the items in the measure dictionary in descending order based on their values.\n",
    "  sorted_x = sorted(cmeasure.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "  #* Check the 'overwrite' flag in the centrality dictionary.\n",
    "  #* If it's True, open the file in write mode, which overwrites the existing content.\n",
    "  #* If it's False, open the file in append mode, which adds to the existing content.\n",
    "  if centrality[\"overwrite\"]:\n",
    "      f = open(filename + centrality[\"prefix\"] + \".txt\", 'w')\n",
    "  else:\n",
    "      f = open(filename + centrality[\"prefix\"] + \".txt\", 'a')\n",
    "  \n",
    "  #* Write the sorted items to the file.\n",
    "  f.write(str(sorted_x))\n",
    "  \n",
    "  #* Close the file.\n",
    "  f.close()\n",
    "\n",
    "  #* Draw the weighted graph with the measure and the name from the centrality dictionary.\n",
    "  draw(G_weighted, centrality[\"measure\"], centrality[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "##### Removing the generic users from the dataset: \"7505d64a54e061b7acd54ccd58b49dc43500b635\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing: 49493\n",
      "Number of rows after removing: 46920\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv('C:/Users/boyma/OneDrive/Desktop/SNA_code/agent_to_tasks/raw_data.csv')\n",
    "\n",
    "# Print the number of rows before removing\n",
    "print(f\"Number of rows before removing: {len(df)}\")\n",
    "\n",
    "# Remove rows where 'receiverid' is \"7505d64a54e061b7acd54ccd58b49dc43500b635\"\n",
    "df = df[df['receiverid'] != \"7505d64a54e061b7acd54ccd58b49dc43500b635\"]\n",
    "\n",
    "# Print the number of rows after removing\n",
    "print(f\"Number of rows after removing: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation \n",
    "- Transform the data from \"reactions\" column into their string equivalents in a new column called \"emoji\"\n",
    "- Transform the data from \"roomid\" column into a new column called \"room_name\". For every new data, append the string \"room_\" to the beginning of the \"roomid\". For example, if the \"roomid\" is \"a0dc6db1830d89519e8f\", then the new column will be \"room_a0dc6db\". \n",
    "- Add a new column called \"commented\" where a string \"commented\" is added when the \"commenter\" column is not empty, otherwise, leave it as is. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from integers to emoji names\n",
    "reaction_mapping = {\n",
    "    1: 'like',\n",
    "    2: 'happy',\n",
    "    3: 'surprise',\n",
    "    4: 'sad',\n",
    "    5: 'angry'\n",
    "}\n",
    "\n",
    "# Create a new column 'emoji' by mapping the 'reaction' column to the corresponding emoji names\n",
    "df['emoji'] = df['reaction'].map(reaction_mapping)\n",
    "\n",
    "# Create a new column 'room_name' by appending 'room_' to the first 7 characters of the 'roomid' column\n",
    "df['room_name'] = 'room_' + df['roomid'].str.slice(0, 7)\n",
    "\n",
    "# Create a new column 'commented' where its data is derived from the 'commenter' column\n",
    "df['commented'] = df['commenter'].apply(lambda x: 'commented' if pd.notnull(x) else '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to a new csv file to check whether the transformation happens properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('new_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def anonymize_id(id):\n",
    "    if pd.isnull(id):\n",
    "        return ''\n",
    "    prefix = id[0]\n",
    "    hash_object = hashlib.sha1(id.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    if prefix == 'A':\n",
    "        return 'adm' + hex_dig[:5]\n",
    "    elif prefix == 'F':\n",
    "        return 'fac' + hex_dig[:5]\n",
    "    elif prefix == 'S':\n",
    "        return 'stu' + hex_dig[:5]\n",
    "    elif prefix == 'P':\n",
    "        return 'par' + hex_dig[:5]\n",
    "    else:\n",
    "        return 'unk' + hex_dig[:5]\n",
    "\n",
    "# Create a dictionary to store the original IDs and their corresponding anonymized IDs\n",
    "anonymized_ids = {}\n",
    "\n",
    "# Get all unique IDs in the 'initiatorid', 'receiverid', 'reactor', and 'commenter' columns\n",
    "unique_ids = pd.concat([df['initiatorid'], df['receiverid'], df['reactor'], df['commenter']]).dropna().unique()\n",
    "\n",
    "# Create a mapping from the original IDs to the hashed IDs\n",
    "for id in unique_ids:\n",
    "    anonymized_ids[id] = anonymize_id(id)\n",
    "\n",
    "# Replace the original IDs with the hashed IDs in the 'initiatorid', 'receiverid', 'reactor', and 'commenter' columns\n",
    "for column in ['initiatorid', 'receiverid', 'reactor', 'commenter']:\n",
    "    df[column] = df[column].map(anonymized_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to a new csv to check whether the anonymization is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('anon_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Mapping The Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "room_5896e22\n",
      "room_07a0beb\n",
      "room_45d026b\n",
      "room_7eb680a\n",
      "room_a0dc6db\n",
      "room_73fc6b9\n",
      "room_21ad9f3\n",
      "room_3814351\n",
      "room_9cebe5e\n",
      "room_09e404c\n",
      "room_bbc5def\n",
      "room_6705f3c\n",
      "room_fa4aa54\n",
      "room_43a3584\n",
      "room_5912cff\n",
      "room_9c852f0\n",
      "room_7711bfe\n",
      "room_2d57440\n",
      "room_51aac1d\n",
      "room_ee69103\n",
      "room_9528233\n",
      "room_7ba0b2d\n",
      "room_d7a297d\n",
      "room_b68c148\n",
      "room_c9540e9\n",
      "room_c97d3de\n",
      "room_5384ea4\n",
      "room_a4ae8a2\n",
      "room_c7b41e1\n",
      "room_1d36d69\n",
      "room_8496ef0\n",
      "room_21357a9\n",
      "room_01a8a99\n",
      "room_adbc112\n",
      "room_7f4db2e\n",
      "room_c8bbfe9\n",
      "room_39aee10\n",
      "room_675a7e3\n",
      "room_8d6af15\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Iterate over the rows of the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the IDs are not empty or null\n",
    "    if pd.notnull(row['initiatorid']) and pd.notnull(row['receiverid']):\n",
    "        # For Timeline Interactions\n",
    "        G.add_edge(row['initiatorid'], row['tltype'])\n",
    "        G.add_edge(row['tltype'], row['receiverid'])\n",
    "\n",
    "        # For Reactions\n",
    "        if pd.notnull(row['reactor']):\n",
    "            if row['reactor'] == row['initiatorid']:\n",
    "                # Add edges from reactor to reaction and from reaction to receiver\n",
    "                G.add_edge(row['reactor'], row['emoji'])\n",
    "                G.add_edge(row['emoji'], row['receiverid'])\n",
    "            elif row['reactor'] == row['receiverid']:\n",
    "                # Add edges from reactor to reaction and from reaction to initiator\n",
    "                G.add_edge(row['reactor'], row['emoji'])\n",
    "                G.add_edge(row['emoji'], row['initiatorid'])\n",
    "\n",
    "        # For Comments\n",
    "        if pd.notnull(row['commenter']):\n",
    "            G.add_edge(row['commenter'], row['commented'])\n",
    "            G.add_edge(row['commented'], row['initiatorid'])\n",
    "\n",
    "        # For OnlineClass\n",
    "        G.add_edge(row['initiatorid'], row['room_name'])\n",
    "\n",
    "\n",
    "# Add the 'color' attribute to the nodes based on the node labels\n",
    "color_map = {}\n",
    "for node in G.nodes:\n",
    "    node_str = str(node)\n",
    "    if 'fac' in node_str:\n",
    "        color_map[node] = 'blue'\n",
    "    elif 'stu' in node_str:\n",
    "        color_map[node] = 'green'\n",
    "    elif 'adm' in node_str:\n",
    "        color_map[node] = 'yellow'\n",
    "    elif 'sub' in node_str:\n",
    "        color_map[node] = 'orange'\n",
    "    elif 'isp' in node_str:\n",
    "        color_map[node] = 'red'\n",
    "    elif 'bis' in node_str:\n",
    "        color_map[node] = 'purple'\n",
    "    elif 'par' in node_str:\n",
    "        color_map[node] = 'black'\n",
    "    elif 'sup' in node_str:\n",
    "        color_map[node] = 'aqua'\n",
    "    elif node_str in ['assignment', 'attendance', 'Bulletin Board', 'collaborate', 'enroll', 'event', 'grade', 're-assign', 'register', 'like', 'happy', 'surprise', 'sad','angry', 'room_', 'commented']:\n",
    "        color_map[node] = 'gray'\n",
    "    else:\n",
    "        print(node_str)\n",
    "\n",
    "nx.set_node_attributes(G, color_map, 'color')\n",
    "\n",
    "# Add the 'weight' attribute to the edges\n",
    "for u, v, data in G.edges(data=True):\n",
    "    data['weight'] = data.get('weight', 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the graph to a GraphML file\n",
    "nx.write_graphml_lxml(G, 'learning_interactions.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# import hashlib\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv('C:/Users/boyma/OneDrive/Desktop/SNA_code/agent_to_tasks/timeline_dataset.csv')\n",
    "\n",
    "# def anonymize_id(id):\n",
    "#     if pd.isnull(id):\n",
    "#         return 'unknown'\n",
    "#     prefix = id[0]\n",
    "#     hash_object = hashlib.sha1(id.encode())\n",
    "#     hex_dig = hash_object.hexdigest()\n",
    "#     if prefix == 'A':\n",
    "#         return 'adm' + hex_dig[:5]\n",
    "#     elif prefix == 'F':\n",
    "#         return 'fac' + hex_dig[:5]\n",
    "#     elif prefix == 'S':\n",
    "#         return 'stu' + hex_dig[:5]\n",
    "#     elif prefix == 'P':\n",
    "#         return 'par' + hex_dig[:5]\n",
    "#     else:\n",
    "#         return 'unk' + hex_dig[:5]\n",
    "\n",
    "# # Anonymize all the rows or records from the columns: initiatorid, receiverid\n",
    "# df['initiatorid'] = df['initiatorid'].apply(anonymize_id)\n",
    "# df['receiverid'] = df['receiverid'].apply(anonymize_id)\n",
    "\n",
    "\n",
    "# # Create a new directed graph\n",
    "# G = nx.DiGraph()\n",
    "\n",
    "# # Define a color map based on the node labels\n",
    "# color_map = {\n",
    "#     'fac': 'blue',\n",
    "#     'stu': 'green',\n",
    "#     'adm': 'yellow',\n",
    "#     'sub': 'orange',\n",
    "#     'isp': 'red',\n",
    "#     'bis': 'purple',\n",
    "#     'par': 'black',\n",
    "#     'sup': 'aqua',\n",
    "#     'assignment': 'white',\n",
    "#     'attendance': 'white',\n",
    "#     'Bulletin Board': 'white',\n",
    "#     'collaborate': 'white',\n",
    "#     'enroll': 'white',\n",
    "#     'event': 'white',\n",
    "#     'grade': 'white',\n",
    "#     're-assign': 'white',\n",
    "#     'register': 'white'\n",
    "# }\n",
    "\n",
    "# # Iterate over the DataFrame\n",
    "# for index, row in df.iterrows():\n",
    "#     G.add_edge(row['initiatorid'], row['tltype'])\n",
    "#     G.add_edge(row['tltype'], row['receiverid'])\n",
    "\n",
    "#     # Add the color attribute to the nodes\n",
    "#     for node in [row['initiatorid'], row['tltype'], row['receiverid']]:\n",
    "#         for key in color_map:\n",
    "#             if key in node:\n",
    "#                 nx.set_node_attributes(G, {node: color_map[key]}, 'colour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml_lxml(G, 'timeline_interactions.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reactions Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# import hashlib\n",
    "\n",
    "# # Step 1: Import the dataset\n",
    "# df = pd.read_csv('C:/Users/boyma/OneDrive/Desktop/SNA_code/agent_to_tasks/reactions_dataset.csv')\n",
    "\n",
    "# # Step 2: Anonymize the initiatorid, receiverid, reactor\n",
    "# def anonymize_id(id):\n",
    "#     if pd.isnull(id):\n",
    "#         return 'unknown'\n",
    "#     prefix = id[0]\n",
    "#     hash_object = hashlib.sha1(id.encode())\n",
    "#     hex_dig = hash_object.hexdigest()\n",
    "#     if prefix == 'A':\n",
    "#         return 'adm' + hex_dig[:5]\n",
    "#     elif prefix == 'F':\n",
    "#         return 'fac' + hex_dig[:5]\n",
    "#     elif prefix == 'S':\n",
    "#         return 'stu' + hex_dig[:5]\n",
    "#     elif prefix == 'P':\n",
    "#         return 'par' + hex_dig[:5]\n",
    "#     else:\n",
    "#         return 'unk' + hex_dig[:5]\n",
    "\n",
    "# df['initiatorid'] = df['initiatorid'].apply(anonymize_id)\n",
    "# df['receiverid'] = df['receiverid'].apply(anonymize_id)\n",
    "\n",
    "# # Anonymize reactor based on the presence in initiatorid or receiverid\n",
    "# df['reactor'] = df.apply(lambda row: row['initiatorid'] if row['reactor'] == row['initiatorid'] else (row['receiverid'] if row['reactor'] == row['receiverid'] else anonymize_id(row['reactor'])), axis=1)\n",
    "\n",
    "# # Step 3: Generate a Digraph\n",
    "# G = nx.DiGraph()\n",
    "\n",
    "# # Define a color map based on the node labels\n",
    "# color_map = {\n",
    "#     'fac': 'blue',\n",
    "#     'stu': 'green',\n",
    "#     'adm': 'yellow',\n",
    "#     'sub': 'orange',\n",
    "#     'isp': 'red',\n",
    "#     'bis': 'purple',\n",
    "#     'par': 'black',\n",
    "#     'sup': 'aqua',\n",
    "#     'assignment': 'white',\n",
    "#     'attendance': 'white',\n",
    "#     'Bulletin Board': 'white',\n",
    "#     'collaborate': 'white',\n",
    "#     'enroll': 'white',\n",
    "#     'event': 'white',\n",
    "#     'grade': 'white',\n",
    "#     're-assign': 'white',\n",
    "#     'register': 'white'\n",
    "# }\n",
    "\n",
    "# # Step 4: Map their interaction\n",
    "# for index, row in df.iterrows():\n",
    "#     # For Reactions\n",
    "#     if row['reactor'] == row['initiatorid']:\n",
    "#         # Add edges from reactor to reaction and from reaction to receiver\n",
    "#         G.add_edge(row['reactor'], row['reaction'])\n",
    "#         G.add_edge(row['reaction'], row['receiverid'])\n",
    "#     elif row['reactor'] == row['receiverid']:\n",
    "#         # Add edges from reactor to reaction and from reaction to initiator\n",
    "#         G.add_edge(row['reactor'], row['reaction'])\n",
    "#         G.add_edge(row['reaction'], row['initiatorid'])\n",
    "\n",
    "#     # Add the color attribute to the nodes\n",
    "#     for node in [row['initiatorid'], row['reaction'], row['receiverid']]:\n",
    "#         for key in color_map:\n",
    "#             if key in node:\n",
    "#                 nx.set_node_attributes(G, {node: color_map[key]}, 'colour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml_lxml(G, 'reactions_interactions.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import hashlib\n",
    "\n",
    "# def anonymize_id(id):\n",
    "#     if pd.isnull(id):\n",
    "#         return 'unknown'\n",
    "#     prefix = id[0]\n",
    "#     hash_object = hashlib.sha1(id.encode())\n",
    "#     hex_dig = hash_object.hexdigest()\n",
    "#     if prefix == 'A':\n",
    "#         return 'adm' + hex_dig[:5]\n",
    "#     elif prefix == 'F':\n",
    "#         return 'fac' + hex_dig[:5]\n",
    "#     elif prefix == 'S':\n",
    "#         return 'stu' + hex_dig[:5]\n",
    "#     elif prefix == 'P':\n",
    "#         return 'par' + hex_dig[:5]\n",
    "#     else:\n",
    "#         return 'unk' + hex_dig[:5]\n",
    "\n",
    "# # Import the dataset into a DataFrame\n",
    "# df = pd.read_csv('C:/Users/boyma/OneDrive/Desktop/SNA_code/agent_to_tasks/learning_interaction_dataset.csv')\n",
    "\n",
    "# # Drop all records that has this value from the receiverid column\n",
    "# df = df[df['receiverid'] != \"7505d64a54e061b7acd54ccd58b49dc43500b635\"]\n",
    "\n",
    "# # Anonymize all the rows or records from the columns: initiatorid, receiverid\n",
    "# df['initiatorid'] = df['initiatorid'].apply(anonymize_id)\n",
    "# df['receiverid'] = df['receiverid'].apply(anonymize_id)\n",
    "# df['reactor'] = df['reactor'].apply(anonymize_id)\n",
    "\n",
    "# # After the anonymization, transform all records from the reaction column into \"reaction\"\n",
    "# df['reaction'] = 'reaction'\n",
    "\n",
    "# # Also transform all data from the commenter column into \"comment\"\n",
    "# df['commenter'] = 'comment'\n",
    "\n",
    "# # Also, transform all the data from the tltype into \"onlineclass\" except for specific records\n",
    "# df.loc[~df['tltype'].isin(['assignment', 'attendance', 'Bulletin Board', 'collaborate', 'enroll', 'event', 'grade', 're-assign', 'register']), 'tltype'] = 'onlineclass'\n",
    "\n",
    "# # Create a Directed Graph out of the new dataframe\n",
    "# G = nx.DiGraph()\n",
    "\n",
    "# # Define a color map based on the node labels\n",
    "# color_map = {\n",
    "#     'fac': 'blue',\n",
    "#     'stu': 'green',\n",
    "#     'adm': 'yellow',\n",
    "#     'sub': 'orange',\n",
    "#     'isp': 'red',\n",
    "#     'bis': 'purple',\n",
    "#     'par': 'black',\n",
    "#     'sup': 'aqua',\n",
    "#     'assignment': 'white',\n",
    "#     'attendance': 'white',\n",
    "#     'Bulletin Board': 'white',\n",
    "#     'collaborate': 'white',\n",
    "#     'enroll': 'white',\n",
    "#     'event': 'white',\n",
    "#     'grade': 'white',\n",
    "#     're-assign': 'white',\n",
    "#     'register': 'white',\n",
    "#     'reaction': 'white',\n",
    "#     'comment': 'white',\n",
    "#     'onlineclass': 'white'\n",
    "# }\n",
    "\n",
    "# # Add edges to the graph based on the interactions and apply the color mapping\n",
    "# for index, row in df.iterrows():\n",
    "#     # For Timeline Interactions\n",
    "#     G.add_edge(row['initiatorid'], row['tltype'])\n",
    "#     G.add_edge(row['tltype'], row['receiverid'])\n",
    "\n",
    "#     # For Reactions\n",
    "#     if row['reactor'] == row['initiatorid']:\n",
    "#         # Add edges from reactor to reaction and from reaction to receiver\n",
    "#         G.add_edge(row['reactor'], row['reaction'])\n",
    "#         G.add_edge(row['reaction'], row['receiverid'])\n",
    "#     elif row['reactor'] == row['receiverid']:\n",
    "#         # Add edges from reactor to reaction and from reaction to initiator\n",
    "#         G.add_edge(row['reactor'], row['reaction'])\n",
    "#         G.add_edge(row['reaction'], row['initiatorid'])\n",
    "\n",
    "#     # For Comments\n",
    "#     G.add_edge(row['receiverid'], row['commenter'])\n",
    "#     G.add_edge(row['commenter'], row['initiatorid'])\n",
    "\n",
    "#     # For OnlineClass\n",
    "#     G.add_edge(row['initiatorid'], row['tltype'])\n",
    "\n",
    "#     # Add the color attribute to the nodes\n",
    "#     for node in [row['initiatorid'], row['tltype'], row['receiverid']]:\n",
    "#         for key in color_map:\n",
    "#             if key in node:\n",
    "#                 nx.set_node_attributes(G, {node: color_map[key]}, 'colour')\n",
    "\n",
    "# # Draw the graph\n",
    "# color_values = [data['colour'] for node, data in G.nodes(data=True)]\n",
    "# nx.draw(G, node_color=color_values, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to CSV\n",
    "# df.to_csv('C:/Users/boyma/OneDrive/Desktop/SNA_code/agent_to_tasks/new_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml_lxml(G, 'learning_interactions.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the raw CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import hashlib\n",
    "# from collections import defaultdict\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "\n",
    "# def anonymize_id(id):\n",
    "#     prefix = id[0]\n",
    "#     year = id[1:5]\n",
    "#     hash_object = hashlib.sha1(id.encode())\n",
    "#     hex_dig = hash_object.hexdigest()\n",
    "#     if prefix == 'A':\n",
    "#         return 'adm' + hex_dig[:5]\n",
    "#     elif prefix == 'F':\n",
    "#         return 'fac' + hex_dig[:5]\n",
    "#     elif prefix == 'S':\n",
    "#         return 'stu' + hex_dig[:5]\n",
    "#     elif prefix == 'P':\n",
    "#         return 'par' + hex_dig[:5]\n",
    "#     else:\n",
    "#         return 'unk' + hex_dig[:5]\n",
    "\n",
    "# filename = input(\"Filename:\")\n",
    "# with open(filename + \".csv\", 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     next(reader)\n",
    "\n",
    "#     interactions = defaultdict(int)\n",
    "#     rows = []\n",
    "#     for row in reader:\n",
    "#         initiatorid = anonymize_id(row[0])\n",
    "#         receiverid = anonymize_id(row[1])\n",
    "#         tltype = row[2]\n",
    "#         interactions[(initiatorid, tltype)] += 1\n",
    "#         interactions[(tltype, receiverid)] += 1\n",
    "#         rows.append([initiatorid, tltype, receiverid])\n",
    "\n",
    "# # Create a DataFrame from the rows\n",
    "# df = pd.DataFrame(rows, columns=['initiatorid', 'tltype', 'receiverid'])\n",
    "\n",
    "# # Add a 'weight' column to the DataFrame\n",
    "# df['weight'] = df.apply(lambda row: interactions[(row['initiatorid'], row['tltype'])] + interactions[(row['tltype'], row['receiverid'])], axis=1)\n",
    "\n",
    "# # Now 'df' is a DataFrame that contains your preprocessed data.\n",
    "# initiatorid = df['initiatorid']\n",
    "# tltype = df['tltype']\n",
    "# receiverid = df['receiverid']\n",
    "# weight = df['weight']\n",
    "\n",
    "# # Create a new weighted graph.\n",
    "# G_weighted = nx.Graph()\n",
    "\n",
    "# # For each row in the DataFrame, add edges to the graph.\n",
    "# # The nodes are the 'initiatorid', 'tltype', and 'receiverid' columns of the row.\n",
    "# # The weight of the edge is the 'weight' column of the row.\n",
    "# for index, row in df.iterrows():\n",
    "#     G_weighted.add_edge(row['initiatorid'], row['tltype'], weight=row['weight'])\n",
    "#     G_weighted.add_edge(row['tltype'], row['receiverid'], weight=row['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Plot the graph using the plot_Gp function.\n",
    "#plot_Gp(G_weighted, measures=nx.degree_centrality(G_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Plot the graph using the plot_Gp function.\n",
    "#plot_Gp(G_weighted, measures=nx.betweenness_centrality(G_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Plot the graph using the plot_Gp function.\n",
    "#plot_Gp(G_weighted, measures=nx.eigenvector_centrality(G_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Generate a graphml version of this graph: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Topological Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Print the graph.\n",
    "# print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the degree centrality of the graph.\n",
    "# degree = nx.degree_centrality(G_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Draw the graph with node sizes proportional to their degree centrality.\n",
    "# draw(G_weighted, nx.degree_centrality(G_weighted), 'Degree Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Sort the nodes by their degree centrality and print the sorted list.\n",
    "# x = degree\n",
    "# sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n",
    "# for item in sorted_x:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Write the eigenvector centrality to a file and render it.\n",
    "# writeandrender(\n",
    "#     filename, {\n",
    "#         \"measure\": degree,\n",
    "#         \"name\": \"Degree Centrality\",\n",
    "#         \"prefix\": \"degree\",\n",
    "#         \"overwrite\": True\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the eigenvector centrality of the graph.\n",
    "# eigenvector = nx.eigenvector_centrality(G_weighted, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Draw the graph with node sizes proportional to their eigenvector centrality.\n",
    "# draw(G_weighted, eigenvector, 'Eigenvector Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Sort the nodes by their betweenness centrality and print the sorted list.\n",
    "# x = eigenvector\n",
    "# sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n",
    "# for item in sorted_x:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Write the eigenvector centrality to a file.\n",
    "# with open(filename + \"eigen.txt\", 'a') as f:\n",
    "#     f.write(str(eigenvector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Write the eigenvector centrality to a file and render it.\n",
    "# writeandrender(filename,\n",
    "#     {\n",
    "#         \"measure\":eigenvector,\n",
    "#         \"name\": \"Eigenvector Centrality\",\n",
    "#         \"prefix\":\"eigen\",\n",
    "#         \"overwrite\": True\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the betweenness centrality of the graph.\n",
    "# betweenness = nx.betweenness_centrality(G_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Draw the graph with node sizes proportional to their betweenness centrality.\n",
    "# draw(G_weighted, betweenness, 'Betweenness Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Sort the nodes by their betweenness centrality and print the sorted list.\n",
    "# x = nx.betweenness_centrality(G_weighted)\n",
    "# sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n",
    "# for item in sorted_x:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Write the betweenness centrality to a file and render it.\n",
    "# writeandrender(\n",
    "#   filename, {\n",
    "#       \"measure\": betweenness,\n",
    "#       \"name\": 'Betweenness Centrality',\n",
    "#       \"prefix\": \"bet\",\n",
    "#       \"overwrite\": True\n",
    "#   }\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
