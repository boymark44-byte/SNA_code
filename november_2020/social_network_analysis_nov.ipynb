{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from networkx_viewer import Viewer\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict\n",
    "import networkx as nx \n",
    "\n",
    "#from model import spcall\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(G, measures, measure_name):\n",
    "  #* https://stackoverflow.com/a/52013202\n",
    "  #* https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html\n",
    "  #* https://www.datacamp.com/community/tutorials/social-network-analysis-python\n",
    "\n",
    "  #* Create two lists of edges based on their weight.\n",
    "  #* 'elarge' contains edges with weight greater than 5.\n",
    "  #* 'esmall' contains edges with weight less than or equal to 5.\n",
    "  elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 5]\n",
    "  esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 5]\n",
    "\n",
    "  #* Generate a spring layout for the graph.\n",
    "  pos = nx.spring_layout(G)\n",
    "\n",
    "  #* Set the size of each node based on its corresponding measure value.\n",
    "  node_size = [v * 1000 for v in measures.values()]\n",
    "\n",
    "  #* Draw the nodes of the graph with their size and color determined by the measure values.\n",
    "  #* The color map 'plt.cm.plasma' is used for coloring the nodes.\n",
    "  nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size, \n",
    "                                  cmap=plt.cm.plasma,\n",
    "                                  node_color=list(measures.values()),\n",
    "                                  nodelist=measures.keys())\n",
    "\n",
    "  #* Set the color normalization of the nodes to be logarithmic.\n",
    "  nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n",
    "  \n",
    "  #* Draw the edges of the graph.\n",
    "  edges = nx.draw_networkx_edges(G, pos)\n",
    "  \n",
    "  #* Draw the 'elarge' and 'esmall' edges with different styles.\n",
    "  #* The 'elarge' edges are drawn with a width of 2.\n",
    "  #* The 'esmall' edges are drawn with a width of 2, transparency of 0.5, blue color, and dashed style.\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=elarge, width=2)\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=esmall, width=2, alpha=0.5, edge_color='blue', style='dashed')\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, blue color, and sans-serif font family.\n",
    "  #nx.draw_networkx_labels(G, pos, font_size=10, font_color='blue', font_family='sans-serif')\n",
    "  \n",
    "  #* Set the title of the plot, add a color bar, turn off the axis, and display the plot.\n",
    "  plt.title(measure_name)\n",
    "  plt.colorbar(nodes)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_G(G, measures):\n",
    "  #* Define two lists of edges based on their weight.\n",
    "  #* 'elarge' contains edges with weight greater than 5.\n",
    "  #* 'esmall' contains edges with weight less than or equal to 5.\n",
    "  elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 5]\n",
    "  esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 5]\n",
    "\n",
    "  #* Generate a spring layout for the graph.\n",
    "  #* This layout treats edges as springs holding nodes close, while treating nodes as repelling objects.\n",
    "  pos = nx.spring_layout(G)\n",
    "  \n",
    "  #* Set the size of each node based on its corresponding measure value.\n",
    "  #* The size is multiplied by 1000 for better visibility.\n",
    "  node_size = [v * 1000 for v in measures.values()]\n",
    "\n",
    "  #* Draw the nodes of the graph with their size and color determined by the measure values.\n",
    "  #* The color map 'plt.cm.plasma' is used for coloring the nodes.\n",
    "  nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,\n",
    "                                cmap=plt.cm.plasma,\n",
    "                                node_color=list(measures.values()),\n",
    "                                nodelist=measures.keys())\n",
    "\n",
    "  #* Set the color normalization of the nodes to be logarithmic.\n",
    "  #* This can be useful if the measure values vary widely.\n",
    "  nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n",
    "\n",
    "  #* Draw the nodes of the graph again with a fixed size of 50 and a color map of 'plt.cm.plasma'.\n",
    "  nx.draw_networkx_nodes(G, pos, node_size=50, cmap=plt.cm.plasma)\n",
    "\n",
    "  #* Draw the 'elarge' and 'esmall' edges with different styles.\n",
    "  #* The 'elarge' edges are drawn with a width of 2.\n",
    "  #* The 'esmall' edges are drawn with a width of 2, transparency of 0.5, blue color, and dashed style.\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=elarge, width=2)\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=esmall, width=2, alpha=0.5, edge_color='blue', style='dashed')\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, black color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G ,pos, font_size=10, font_color='black', font_family='sans-serif')\n",
    "\n",
    "  #* Turn off the axis and display the plot.\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Gp(G, measures):\n",
    "  # Set the figure size to make the plot high-definition.\n",
    "  plt.figure(figsize=(50, 50), dpi=300)\n",
    "\n",
    "  #* Define two lists of edges based on their weight.\n",
    "  #* 'elarge' contains edges with weight greater than 5.\n",
    "  #* 'esmall' contains edges with weight less than or equal to 5.\n",
    "  elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 5]\n",
    "  esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 5]\n",
    "\n",
    "  #* Generate a spring layout for the graph.\n",
    "  #* This layout treats edges as springs holding nodes close, while treating nodes as repelling objects.\n",
    "  pos = nx.spring_layout(G, iterations=13, scale=300, seed=1234)\n",
    "\n",
    "  #* Set the size of each node based on its corresponding measure value.\n",
    "  #* The size is multiplied by 1000 for better visibility.\n",
    "  node_size = [v * 1000 for v in measures.values()]\n",
    "\n",
    "  #* Draw the nodes of the graph with their size and color determined by the measure values.\n",
    "  #* The color map 'plt.cm.plasma' is used for coloring the nodes.\n",
    "  nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size, cmap=plt.cm.plasma, \n",
    "                                  node_color=list(measures.values()),\n",
    "                                  nodelist=measures.keys())\n",
    "\n",
    "  #* Set the color normalization of the nodes to be logarithmic.\n",
    "  #* This can be useful if the measure values vary widely.\n",
    "  nodes.set_norm(mcolors.SymLogNorm(linthresh=0.01, linscale=1))\n",
    "\n",
    "  #* Create a color map based on the node labels.\n",
    "  #* Different labels are mapped to different colors. \n",
    "  # color_map = []\n",
    "  # for node in G:\n",
    "  #   if 'fac' in node:\n",
    "  #       color_map.append('blue')\n",
    "  #   elif 'stu' in node:\n",
    "  #       color_map.append('green')\n",
    "  #   elif 'adm' in node:\n",
    "  #       color_map.append('yellow')\n",
    "  #   elif 'sub' in node:\n",
    "  #       color_map.append('orange')\n",
    "  #   elif 'isp' in node:\n",
    "  #       color_map.append('red')\n",
    "  #   elif 'bis' in node:\n",
    "  #       color_map.append('purple')\n",
    "  #   elif 'par' in node:\n",
    "  #       color_map.append('black')\n",
    "  #   elif 'sup' in node:\n",
    "  #       color_map.append('aqua')\n",
    "  #   elif node in ['grade', 'assignment', 'Bulletin Board', 're-assign', 'enroll', 'register', 'transfer', 'drop']:\n",
    "  #       color_map.append('gray')  # white for the specific nodes\n",
    "  #   else:\n",
    "  #       print (node)\n",
    "\n",
    "\n",
    "  # Add the 'color' attribute to the nodes based on the node labels\n",
    "  color_map = {}\n",
    "  for node in G.nodes:\n",
    "      node_str = str(node)\n",
    "      if 'fac' in node_str:\n",
    "          color_map[node] = 'blue'\n",
    "      elif 'stu' in node_str:\n",
    "          color_map[node] = 'green'\n",
    "      elif 'adm' in node_str:\n",
    "          color_map[node] = 'yellow'\n",
    "      elif 'sub' in node_str:\n",
    "          color_map[node] = 'orange'\n",
    "      elif 'isp' in node_str:\n",
    "          color_map[node] = 'red'\n",
    "      elif 'bis' in node_str:\n",
    "          color_map[node] = 'purple'\n",
    "      elif 'par' in node_str:\n",
    "          color_map[node] = 'black'\n",
    "      elif 'sup' in node_str:\n",
    "          color_map[node] = 'aqua'\n",
    "      elif node_str in ['assignment', 'attendance', 'Bulletin Board', 'collaborate', 'enroll', 'event', 'grade', 're-assign', 'register', 'like', 'happy', 'surprise', 'sad','angry', 'room_', 'commented']:\n",
    "          color_map[node] = 'gray'\n",
    "      else:\n",
    "          print(node_str)\n",
    "\n",
    "  #* Draw the nodes of the graph again with a fixed size of 10 and a color map based on the node labels.\n",
    "  nx.draw_networkx_nodes(G, pos, node_size=10, node_color=color_map, cmap=plt.cm.plasma)\n",
    "\n",
    "  #* Draw the 'elarge' and 'esmall' edges with different styles.\n",
    "  #* The 'elarge' edges are drawn with a width of 2 and gray color.\n",
    "  #* The 'esmall' edges are drawn with a width of 2, transparency of 0.5, gray color, and dashed style.\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=elarge, edge_color='gray', width=1)\n",
    "  nx.draw_networkx_edges(G, pos, edgelist=esmall, width=1, alpha=0.5, edge_color='gray', style='dashed')\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, gray color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G, pos, font_size=3, font_color='black', font_family='sans-serif')\n",
    "\n",
    "  #* Turn off the axis and display the plot.\n",
    "  plt.axis('off')\n",
    "\n",
    "  #* Increase the DPI to 300 for a high-quality plot.\n",
    "  plt.savefig(\"network.png\", dpi=300)\n",
    "  \n",
    "  #* Display the plot \n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_G2(G):\n",
    "  #* Generate a spring layout for the graph.\n",
    "  pos = nx.spring_layout(G)\n",
    "\n",
    "  #* Draw the graph using NetworkX's built-in draw function.\n",
    "  nx.draw_networkx(G)\n",
    "\n",
    "  #* Add labels to the nodes with a font size of 10, gray color, and sans-serif font family.\n",
    "  nx.draw_networkx_labels(G, pos, font_size=10, font_color='gray', font_family='sans-serif')\n",
    "\n",
    "  #* Turn off the axis and display the plot.\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeandrender(filename, centrality):\n",
    "  #* Extract the measure from the centrality dictionary.\n",
    "  cmeasure = centrality[\"measure\"]\n",
    "  \n",
    "  #* Sort the items in the measure dictionary in descending order based on their values.\n",
    "  sorted_x = sorted(cmeasure.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "  #* Check the 'overwrite' flag in the centrality dictionary.\n",
    "  #* If it's True, open the file in write mode, which overwrites the existing content.\n",
    "  #* If it's False, open the file in append mode, which adds to the existing content.\n",
    "  if centrality[\"overwrite\"]:\n",
    "      f = open(filename + centrality[\"prefix\"] + \".txt\", 'w')\n",
    "  else:\n",
    "      f = open(filename + centrality[\"prefix\"] + \".txt\", 'a')\n",
    "  \n",
    "  #* Write the sorted items to the file.\n",
    "  f.write(str(sorted_x))\n",
    "  \n",
    "  #* Close the file.\n",
    "  f.close()\n",
    "\n",
    "  #* Draw the weighted graph with the measure and the name from the centrality dictionary.\n",
    "  draw(G, centrality[\"measure\"], centrality[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Filter for the Month of November 2020\n",
    "\n",
    "##### Removing the generic users from the dataset: \"7505d64a54e061b7acd54ccd58b49dc43500b635\"\n",
    "##### Filtering for the whole month of November 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "cs_dept_computer = 'C:/Users/IIT_C/OneDrive/Desktop/social_network_analysis/SNA_code/raw_data.csv'\n",
    "mark_desktop = 'C:/Users/boyma/OneDrive/Desktop/learning_interactions/SNA_code/raw_data.csv' \n",
    "df = pd.read_csv(mark_desktop)\n",
    "\n",
    "# Convert the 'ts' column to datetime\n",
    "df['ts'] = pd.to_datetime(df['ts'])\n",
    "\n",
    "# Define start and end date\n",
    "start_date = '2020-11-01'\n",
    "end_date = '2020-11-30'\n",
    "\n",
    "# Filter rows based on date range\n",
    "mask = (df['ts'] >= start_date) & (df['ts'] <= end_date)\n",
    "df = df.loc[mask]\n",
    "\n",
    "# Print the number of rows before removing\n",
    "print(f\"Number of rows before removing: {len(df)}\")\n",
    "\n",
    "# Remove rows where 'receiverid' is \"7505d64a54e061b7acd54ccd58b49dc43500b635\"\n",
    "df = df[df['receiverid'] != \"7505d64a54e061b7acd54ccd58b49dc43500b635\"]\n",
    "\n",
    "# Print the number of rows after removing\n",
    "print(f\"Number of rows after removing: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export into CSV to check whether the date filtration is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('filtered_date_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation \n",
    "- Transform the data from \"reactions\" column into their string equivalents in a new column called \"emoji\"\n",
    "- Transform the data from \"roomid\" column into a new column called \"room_name\". For every new data, append the string \"room_\" to the beginning of the \"roomid\". For example, if the \"roomid\" is \"a0dc6db1830d89519e8f\", then the new column will be \"room_a0dc6db\". \n",
    "- Add a new column called \"commented\" where a string \"commented\" is added when the \"commenter\" column is not empty, otherwise, leave it as is. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from integers to emoji names\n",
    "reaction_mapping = {\n",
    "    1: 'like',\n",
    "    2: 'happy',\n",
    "    3: 'surprise',\n",
    "    4: 'sad',\n",
    "    5: 'angry'\n",
    "}\n",
    "\n",
    "# Create a new column 'emoji' by mapping the 'reaction' column to the corresponding emoji names\n",
    "df['emoji'] = df['reaction'].map(reaction_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to a new csv file to check whether the transformation is sucessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('transformed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymize the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def anonymize_id(id):\n",
    "    if pd.isnull(id):\n",
    "        return ''\n",
    "    prefix = id[0]\n",
    "    hash_object = hashlib.sha1(id.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    if prefix == 'A':\n",
    "        return 'adm' + hex_dig[:5]\n",
    "    elif prefix == 'F':\n",
    "        return 'fac' + hex_dig[:5]\n",
    "    elif prefix == 'S':\n",
    "        return 'stu' + hex_dig[:5]\n",
    "    elif prefix == 'P':\n",
    "        return 'par' + hex_dig[:5]\n",
    "    else:\n",
    "        return 'unk' + hex_dig[:5]\n",
    "\n",
    "# Create a dictionary to store the original IDs and their corresponding anonymized IDs\n",
    "anonymized_ids = {}\n",
    "\n",
    "# Get all unique IDs in the 'initiatorid', 'receiverid', 'reactor', and 'commenter' columns\n",
    "unique_ids = pd.concat([df['initiatorid'], df['receiverid'], df['reactor'], df['commenter']]).dropna().unique()\n",
    "\n",
    "# Create a mapping from the original IDs to the hashed IDs\n",
    "for id in unique_ids:\n",
    "    anonymized_ids[id] = anonymize_id(id)\n",
    "\n",
    "# Replace the original IDs with the hashed IDs in the 'initiatorid', 'receiverid', 'reactor', and 'commenter' columns\n",
    "for column in ['initiatorid', 'receiverid', 'reactor', 'commenter']:\n",
    "    df[column] = df[column].map(anonymized_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to a new csv to check whether the anonymization is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('anon_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the Agent to Task Learning Interactions for the whole month of November 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Iterate over the rows of the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the IDs are not empty or null\n",
    "    if pd.notnull(row['initiatorid']) and pd.notnull(row['receiverid']):\n",
    "        # For Timeline Interactions\n",
    "        if G.has_edge(row['initiatorid'], row['tltype']):\n",
    "            G[row['initiatorid']][row['tltype']]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(row['initiatorid'], row['tltype'], weight=1)\n",
    "        \n",
    "        if G.has_edge(row['tltype'], row['receiverid']):\n",
    "            G[row['tltype']][row['receiverid']]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(row['tltype'], row['receiverid'], weight=1)\n",
    "\n",
    "    # For Reactions\n",
    "    if pd.notnull(row['reactor']) and pd.notnull(row['emoji']):\n",
    "        if row['reactor'] == row['initiatorid']:\n",
    "            # Add edges from reactor to reaction and from reaction to receiver\n",
    "            if G.has_edge(row['reactor'], row['emoji']):\n",
    "                G[row['reactor']][row['emoji']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(row['reactor'], row['emoji'], weight=1)\n",
    "            \n",
    "            if G.has_edge(row['emoji'], row['receiverid']):\n",
    "                G[row['emoji']][row['receiverid']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(row['emoji'], row['receiverid'], weight=1)\n",
    "        elif row['reactor'] == row['receiverid']:\n",
    "            # Add edges from reactor to reaction and from reaction to initiator\n",
    "            if G.has_edge(row['reactor'], row['emoji']):\n",
    "                G[row['reactor']][row['emoji']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(row['reactor'], row['emoji'], weight=1)\n",
    "            \n",
    "            if G.has_edge(row['emoji'], row['initiatorid']):\n",
    "                G[row['emoji']][row['initiatorid']]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(row['emoji'], row['initiatorid'], weight=1)\n",
    "\n",
    "    # For Comments\n",
    "    if pd.notnull(row['commenter']) and pd.notnull(row['initiatorid']):\n",
    "        if G.has_edge(row['commenter'], row['initiatorid']):\n",
    "            G[row['commenter']][row['initiatorid']]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(row['commenter'], row['initiatorid'], weight=1)\n",
    "\n",
    "    # For Process Mining\n",
    "    if pd.notnull(row['initiatorid']) and pd.notnull(row['type_']):\n",
    "        if G.has_edge(row['initiatorid'], row['type_']):\n",
    "            G[row['initiatorid']][row['type_']]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(row['initiatorid'], row['type_'], weight=1)\n",
    "\n",
    "# Get a list of all isolated nodes\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "\n",
    "# Print the isolated nodes\n",
    "print(\"Isolated nodes:\", isolated_nodes)\n",
    "\n",
    "\n",
    "# Add the 'color' attribute to the nodes based on the node labels\n",
    "color_map = {}\n",
    "for node in G.nodes:\n",
    "    node_str = str(node)\n",
    "    if 'fac' in node_str:\n",
    "        color_map[node] = '#0000FF'  # Blue\n",
    "    elif 'stu' in node_str:\n",
    "        color_map[node] = '#008000'  # Green\n",
    "    elif 'adm' in node_str:\n",
    "        color_map[node] = '#FFFF00'  # Yellow\n",
    "    elif 'sub' in node_str:\n",
    "        color_map[node] = '#FFA500'  # Orange\n",
    "    elif 'isp' in node_str:\n",
    "        color_map[node] = '#FF0000'  # Red\n",
    "    elif 'bis' in node_str:\n",
    "        color_map[node] = '#800080'  # Purple\n",
    "    elif 'par' in node_str:\n",
    "        color_map[node] = '#00FF00'  # Lime Green\n",
    "    elif 'sup' in node_str:\n",
    "        color_map[node] = '#00FFFF'  # Aqua\n",
    "    elif 'commented' in node_str:\n",
    "        color_map[node] = '#00FFFF'  # Cyan\n",
    "    elif node_str in ['like', 'happy', 'surprise', 'sad', 'angry']:\n",
    "        color_map[node] = '#FFC0CB'  # Pink\n",
    "    elif node_str in ['assignment', 'attendance', 'Bulletin Board', 'collaborate', 'enroll', 'event', 'grade', 're-assign', 'register']:\n",
    "        color_map[node] = '#808080'  # Gray\n",
    "    else:\n",
    "        color_map[node] = '#A52A2A'  # Brown (new color)\n",
    "\n",
    "nx.set_node_attributes(G, color_map, 'color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the Edge Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check edge attributes in a table format\n",
    "def edges_to_dataframe(G):\n",
    "    edge_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        edge_data.append({\n",
    "            'Source': u,\n",
    "            'Target': v,\n",
    "            'Weight': data.get('weight', 1),\n",
    "            'Color': G.nodes[u].get('color', 'unknown') + '-' + G.nodes[v].get('color', 'unknown')\n",
    "        })\n",
    "    return pd.DataFrame(edge_data)\n",
    "\n",
    "# Convert edges to DataFrame\n",
    "edges_df = edges_to_dataframe(G)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(edges_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Nodes with the highest degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree of each node\n",
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "\n",
    "# Sort nodes by degree\n",
    "sorted_degree = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 20 nodes with highest degree\n",
    "top_20_nodes = sorted_degree[:20]\n",
    "\n",
    "# Create DataFrame of top 20 nodes\n",
    "top_20_df = pd.DataFrame(top_20_nodes, columns=['Node', 'Degree'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_20_df)\n",
    "\n",
    "# Plot distribution of degrees of top 20 nodes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_20_df['Node'], top_20_df['Degree'], color='skyblue')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Node')\n",
    "plt.title('Top 20 Nodes with Highest Degree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Graph into a GraphML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml_lxml(G, 'learning_interactions_nov_directed.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Plot the graph using the plot_Gp function.\n",
    "#plot_Gp(G, measures=nx.degree_centrality(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Plot the graph using the plot_Gp function.\n",
    "#plot_Gp(G, measures=nx.betweenness_centrality(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Plot the graph using the plot_Gp function.\n",
    "#plot_Gp(G, measures=nx.eigenvector_centrality(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Topological Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes and Edges\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Density\n",
    "density = nx.density(G)\n",
    "\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the degree centrality of the graph.\n",
    "degree = nx.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Draw the graph with node sizes proportional to their degree centrality.\n",
    "#draw(G, degree, 'Degree Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Sort the nodes by their degree centrality and print the sorted list.\n",
    "x = degree\n",
    "sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n",
    "for item in sorted_x:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes with Top Degree Centrality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sorted nodes and their degree centrality scores into a CSV file.\n",
    "with open('degree_centrality_scores.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Node', 'Degree_Centrality']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for item in sorted_x:\n",
    "        writer.writerow({'Node': item[0], 'Degree_Centrality': item[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the eigenvector centrality of the graph.\n",
    "eigenvector = nx.eigenvector_centrality(G, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Draw the graph with node sizes proportional to their eigenvector centrality.\n",
    "#draw(G, eigenvector, 'Eigenvector Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Sort the nodes by their betweenness centrality and print the sorted list.\n",
    "x = eigenvector\n",
    "sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n",
    "for item in sorted_x:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes with Top Eigenvector Centrality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sorted nodes and their degree centrality scores into a CSV file.\n",
    "with open('eigenvector_centrality_scores.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Node', 'Influence_Centrality']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for item in sorted_x:\n",
    "        writer.writerow({'Node': item[0], 'Influence_Centrality': item[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate the betweenness centrality of the graph.\n",
    "betweenness = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Draw the graph with node sizes proportional to their betweenness centrality.\n",
    "#draw(G, betweenness, 'Betweenness Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Sort the nodes by their betweenness centrality and print the sorted list.\n",
    "x = nx.betweenness_centrality(G)\n",
    "sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n",
    "for item in sorted_x:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes with Top Betweenness Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sorted nodes and their degree centrality scores into a CSV file.\n",
    "with open('betweenness_centrality_scores.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Node', 'Betweenness_Centrality']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for item in sorted_x:\n",
    "        writer.writerow({'Node': item[0], 'Betweenness_Centrality': item[1]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
